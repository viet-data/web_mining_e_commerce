{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from underthesea import word_tokenize\n",
    "from bm25 import BM25Okapi\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_punctuation_vietnamese(text):\n",
    "    # Define Vietnamese punctuation characters\n",
    "    vietnamese_punctuation = \"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"\n",
    "\n",
    "    # Create a translation table\n",
    "    translator = str.maketrans('', '', vietnamese_punctuation)\n",
    "\n",
    "    # Remove punctuation using the translation table and regex\n",
    "    cleaned_text = re.sub(f\"[{re.escape(vietnamese_punctuation)}]\", '', text)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    dict_map = {\n",
    "    \"òa\": \"oà\",\n",
    "    \"Òa\": \"Oà\",\n",
    "    \"ÒA\": \"OÀ\",\n",
    "    \"óa\": \"oá\",\n",
    "    \"Óa\": \"Oá\",\n",
    "    \"ÓA\": \"OÁ\",\n",
    "    \"ỏa\": \"oả\",\n",
    "    \"Ỏa\": \"Oả\",\n",
    "    \"ỎA\": \"OẢ\",\n",
    "    \"õa\": \"oã\",\n",
    "    \"Õa\": \"Oã\",\n",
    "    \"ÕA\": \"OÃ\",\n",
    "    \"ọa\": \"oạ\",\n",
    "    \"Ọa\": \"Oạ\",\n",
    "    \"ỌA\": \"OẠ\",\n",
    "    \"òe\": \"oè\",\n",
    "    \"Òe\": \"Oè\",\n",
    "    \"ÒE\": \"OÈ\",\n",
    "    \"óe\": \"oé\",\n",
    "    \"Óe\": \"Oé\",\n",
    "    \"ÓE\": \"OÉ\",\n",
    "    \"ỏe\": \"oẻ\",\n",
    "    \"Ỏe\": \"Oẻ\",\n",
    "    \"ỎE\": \"OẺ\",\n",
    "    \"õe\": \"oẽ\",\n",
    "    \"Õe\": \"Oẽ\",\n",
    "    \"ÕE\": \"OẼ\",\n",
    "    \"ọe\": \"oẹ\",\n",
    "    \"Ọe\": \"Oẹ\",\n",
    "    \"ỌE\": \"OẸ\",\n",
    "    \"ùy\": \"uỳ\",\n",
    "    \"Ùy\": \"Uỳ\",\n",
    "    \"ÙY\": \"UỲ\",\n",
    "    \"úy\": \"uý\",\n",
    "    \"Úy\": \"Uý\",\n",
    "    \"ÚY\": \"UÝ\",\n",
    "    \"ủy\": \"uỷ\",\n",
    "    \"Ủy\": \"Uỷ\",\n",
    "    \"ỦY\": \"UỶ\",\n",
    "    \"ũy\": \"uỹ\",\n",
    "    \"Ũy\": \"Uỹ\",\n",
    "    \"ŨY\": \"UỸ\",\n",
    "    \"ụy\": \"uỵ\",\n",
    "    \"Ụy\": \"Uỵ\",\n",
    "    \"ỤY\": \"UỴ\",\n",
    "    }\n",
    "    for i, j in dict_map.items():\n",
    "        text = text.replace(i, j)\n",
    "    text = re.sub(r'\\.',' . ',text)\n",
    "    text = re.sub(r'\\,', \"\", text)\n",
    "    text = re.sub(r'\\--', \"\", text)\n",
    "    text = text.strip()\n",
    "    text = \" \".join(text.split())\n",
    "    text = text.lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdataset_df = pd.read_csv(\"testdataset.csv\")\n",
    "test = pd.read_csv(\"test 1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bm25 import BM25Okapi\n",
    "a = test['description'] + \" \" + test['name']\n",
    "corpus = a.dropna().apply(preprocess).apply(remove_punctuation_vietnamese).apply(word_tokenize).tolist()\n",
    "model = BM25Okapi(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_testdataset = testdataset_df['question'].dropna().apply(preprocess).apply(remove_punctuation_vietnamese).apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision@1 = 0.25555555555555554\n",
      "mAP@1 = 0.0627\n",
      "precision@5 = 0.21388888888888935\n",
      "mAP@5 = 0.1766\n",
      "precision@10 = 0.1583333333333335\n",
      "mAP@10 = 0.215\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10]:\n",
    "    test_pred = []\n",
    "    for i, query in enumerate(tokenized_testdataset.tolist()):\n",
    "        # test_pred.append(model.get_top_n(query, corpus, n = len(testdataset_df['product_id'])))\n",
    "        scores = model.get_scores(query)\n",
    "        top_n = np.argsort(scores)[::-1][:k]\n",
    "        test_pred.append(top_n)\n",
    "\n",
    "    def calculate_map(test_Y, pred):\n",
    "\n",
    "        Q = len(test_Y)\n",
    "        ap = []\n",
    "\n",
    "        # loop through and calculate AP for each query q\n",
    "        preds_at_k = []\n",
    "        for q in range(Q):\n",
    "            ap_num = 0\n",
    "            # loop through k values\n",
    "            sum_pred = 0\n",
    "            for x in range(len(pred[q])):\n",
    "                # calculate precision@k\n",
    "                act_set = set(test_Y[q])                                                                                                                                   \n",
    "                pred_set = set(pred[q][:x+1])\n",
    "                precision_at_k = len(act_set & pred_set) / (x+1)\n",
    "                # sum_pred_at_k = len(act_set & pred_set) / (k)\n",
    "                # calculate rel_k values\n",
    "                if pred[q][x] in test_Y[q]:\n",
    "                    rel_k = 1\n",
    "                else:\n",
    "                    rel_k = 0\n",
    "                # calculate numerator value for ap\n",
    "                ap_num += precision_at_k * rel_k\n",
    "            # now we calculate the AP value as the average of AP\n",
    "            # numerator values\n",
    "            ap_q = ap_num / len(test_Y[q])\n",
    "            # print(f\"AP@{len(pred[q])}_{q+1} = {round(ap_q,2)}\")\n",
    "            ap.append(ap_q)\n",
    "            \n",
    "            preds_at_k.append(precision_at_k)\n",
    "\n",
    "        # now we take the mean of all ap values to get mAP\n",
    "        map_at_k = sum(ap) / Q\n",
    "        print(f\"precision@{len(pred[q])} = {sum(preds_at_k)/Q}\")\n",
    "        # generate results\n",
    "        print(f\"mAP@{len(pred[q])} = {round(map_at_k, 4)}\")\n",
    "\n",
    "        return map_at_k\n",
    "\n",
    "    calculate_map(testdataset_df['product_id'].apply(ast.literal_eval).tolist(), test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "WebMining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "0.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
